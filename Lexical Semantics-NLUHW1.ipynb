{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1, DS-GA 1012, Spring 2019\n",
    "\n",
    "## Due Feburary 13, 2019 at 2pm (ET)\n",
    "### Eva Wang sw2860\n",
    "\n",
    "\n",
    "Download the data zip `DS-GA1012-hw1-data.zip`. Complete the following questions in the notebook and submit your completed notebook on NYU Classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring effect of context size [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face many implicit and explicit design decisions in creating distributional word representations. For example, in lecture and in lab, we created word vectors using a co-occurence matrix built on neighboring pairs of words. We might suspect, however, that we can get more signal of word similarity by considering larger contexts than pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Write `build_cooccurrence_matrix`, which generates the co-occurence matrix for a window of arbitrary size and for the vocabulary of `max_vocab_size` most frequent words. Feel free to modify the code used in lab [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading dataset\n",
    "def load_sst(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        data_fh.readline() # skip the header\n",
    "        data = [r.split('\\t')[1] for r in data_fh.readlines()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def build_cooccurrence_matrix(data, max_vocab_size, context_size=1):\n",
    "    \"\"\" Build a co-occurrence matrix\n",
    "    \n",
    "    args:\n",
    "        - data: iterable where each item is a list of tokens (string) \n",
    "        - max_vocab_size: maximum vocabulary size\n",
    "        - context_size: window around a word that is considered context\n",
    "            context_size=1 should consider pairs of adjacent words \n",
    "            \n",
    "    returns:\n",
    "        - co-occurrence matrix: numpy array where row i corresponds to the co-occurrence counts for word i\"\"\"\n",
    "    #raise NotImplementedError\n",
    "    # first, get occurence frequencies of each token, and summrize into a matrix (add 1 count for every word in context range)\n",
    "    def get_token_frequencies():\n",
    "        tok2freq = defaultdict(int)\n",
    "        coocur_counts = defaultdict(lambda: defaultdict(int)) # provide default value for nonexistent key\n",
    "\n",
    "        for datum in data:\n",
    "            tokens = datum.strip().split() # we'll use whitespace to tokenize\n",
    "            for i, tok in enumerate(tokens):\n",
    "                tok2freq[tok] += 1\n",
    "                for j in range(i, min(i+context_size+1,len(tokens))):\n",
    "                    #coocur_counts[tok][tok] += 1\n",
    "                    coocur_counts[tok][tokens[j]] += 1\n",
    "                    coocur_counts[tokens[j]][tok] += 1\n",
    "        return tok2freq, coocur_counts\n",
    "    \n",
    "    def prune_vocabulary(tok2freq, max_vocab_size):\n",
    "        \"\"\" Prune vocab by taking max_vocab_size most frequent words to reduce dimension \"\"\"\n",
    "        tok_and_freqs = [(k, v) for k, v in tok2freq.items()]\n",
    "        tok_and_freqs.sort(key = lambda x: x[1], reverse=True) # sorts in-place\n",
    "        tok2idx = {tok: idx for idx, (tok, _) in enumerate(tok_and_freqs[:max_vocab_size])}\n",
    "        idx2tok = {idx: tok for tok, idx in tok2idx.items()}\n",
    "        return tok2idx, idx2tok\n",
    "    \n",
    "    def _build_coocurrence_mat(idx2tok, coocur_counts):\n",
    "        #mat = [[coocur_counts[idx2tok[i]][idx2tok[j]] for j in range(len(idx2tok))] for i in range(len(idx2tok))]\n",
    "        vocab_size = len(idx2tok)\n",
    "        mat = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
    "        for i in range(vocab_size - 1):\n",
    "            for j in range(i+1, vocab_size):\n",
    "                if coocur_counts[idx2tok[i]][idx2tok[j]]:\n",
    "                    mat[i][j] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "                    mat[j][i] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "        # fill in the coocurrence matrix with coocur counts (in the range of top frequencies) coverted idx2tok\n",
    "        return np.array(mat)\n",
    "        \n",
    "    print(\"Counting words...\")\n",
    "    start_time = time.time()\n",
    "    tok2freq, coocur_counts = get_token_frequencies()\n",
    "    print(\"\\tFinished counting words in %.5f\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"Pruning vocabulary...\")\n",
    "    tok2idx, idx2tok = prune_vocabulary(tok2freq, max_vocab_size)\n",
    "    start_time = time.time()\n",
    "    print(\"\\tFinished pruning vocabulary in %.5f\" % (time.time() - start_time))\n",
    "    \n",
    "    print(\"Building co-occurrence matrix...\")\n",
    "    start_time = time.time()\n",
    "    coocur_mat = _build_coocurrence_mat(idx2tok, coocur_counts)\n",
    "    print(\"\\tFinished building co-occurrence matrix in %.5f\" % (time.time() - start_time))\n",
    "    return coocur_mat, tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of `build_cooccurrence_matrix` to generate the co-occurence matrix from the sentences of [SST](http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip) (file `datasetSentences.txt`) with `context_size=2` and `max_vocab_size=10000`. What is the co-occurrence count of the words \"the\" and \"end\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting words in 2.22808\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 28.34749\n",
      "index of 'the' 2\n",
      "index of 'end' 201\n",
      "co-occurrence count of the words 'the' and 'end': 98\n"
     ]
    }
   ],
   "source": [
    "data_file = 'datasetSentences.txt'\n",
    "data = load_sst(data_file)\n",
    "# find the co-occurence count of \"the\" and \"end\"\n",
    "coocur_mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=2)\n",
    "\n",
    "print(\"index of 'the'\", tok2idx[\"the\"])\n",
    "print(\"index of 'end'\", tok2idx[\"end\"])\n",
    "print(\"co-occurrence count of the words 'the' and 'end':\", coocur_mat[2][201])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Plot the effect of varying context size in $\\{1, 2, 3, 4\\}$ (leaving all the other settings the same) on the quality of the learned word embeddings, as measured by performance (Spearman correlation) on the word similarity dataset [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) between human judgments and cosine similarity of the learned word vectors (see lab). [12 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
    "    same dimensions as `u`.\"\"\"\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation: use cosine similarity\n",
    "def load_word_similarity_dataset(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        raw_data = data_fh.readlines()\n",
    "    data = []\n",
    "    trgs = []\n",
    "    for datum in raw_data:\n",
    "        datum = datum.strip().split(',')\n",
    "        data.append((datum[0], datum[1]))\n",
    "        trgs.append(float(datum[2]))\n",
    "    return data, trgs\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
    "    \"\"\" \"\"\"\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    n_exs = 0\n",
    "    for (word1, word2), trg in zip(word_pairs, targets):\n",
    "        if word1 in tok2idx and word2 in tok2idx:\n",
    "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]]) # cosine similarity\n",
    "            preds.append(pred_sim)\n",
    "            trgs.append(trg)\n",
    "            n_exs += 1\n",
    "    \n",
    "    rho, pvalue = spearmanr(trgs, preds) # spearsman correlation \n",
    "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
    "    return rho\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting words in 0.58497\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 27.25204\n",
      "Evaluated on 248 of 771 examples\n",
      "Counting words...\n",
      "\tFinished counting words in 0.61488\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 26.89396\n",
      "Evaluated on 248 of 771 examples\n",
      "Counting words...\n",
      "\tFinished counting words in 0.89776\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 25.82140\n",
      "Evaluated on 248 of 771 examples\n",
      "Counting words...\n",
      "\tFinished counting words in 0.97620\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 25.49470\n",
      "Evaluated on 248 of 771 examples\n"
     ]
    }
   ],
   "source": [
    "test_file = 'MTURK-771.csv'\n",
    "test_data, test_trgs = load_word_similarity_dataset(test_file) #separate data pairs and socres\n",
    "#coocur_mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=2)\n",
    "#evaluate_word_similarity(test_data, test_trgs, coocur_mat, tok2idx)\n",
    "import matplotlib.pyplot as plt\n",
    "context_sizes = [1,2,3,4]\n",
    "rho_list = []\n",
    "for window in context_sizes:\n",
    "    coocur_mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=window)\n",
    "    rho = evaluate_word_similarity(test_data, test_trgs, coocur_mat, tok2idx)\n",
    "    rho_list.append(rho)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'context_sizes')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(context_sizes,rho_list) \n",
    "plt.ylabel('performance: Spearman correlation')\n",
    "plt.xlabel('context_sizes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Briefly discuss the pros and cons of varying (i) the context size (ii) the vocabulary size (iii) using bigrams instead of unigrams (iv) using subword tokens instead of words. [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i)__.  context size \n",
    "    - Closer words are generally considered to be more relevant to a word's meaning, so a larger context size will capture more information. \n",
    "    - However, when the context size getting too large, word embedding will be affected by irrelevent neighbours.\n",
    "    - Smaller context size also makes computations faster.\n",
    "    - Thus, the key is to find the optimal point, which in our case, the word embedding performs best when context size = 3 \n",
    "    \n",
    "__(ii)__. vocabulary size\n",
    "    - Compared to small vocabulary, larger vocabulary generally has less bias and better perforamnce for understanding language, but takes more computation.\n",
    "\n",
    "__(iii)__. use bigrams:    \n",
    "    - Pros: Understand pairs of words by incorporting neighbor words \n",
    "    - Cons: too many infrequent bigrams can be noisy (many of the bi-grams never occur). Higher computational cost\n",
    "\n",
    "__(iv)__. use subword tokens\n",
    "    - Pros: take sub-word information into account. Character embedding is especially effective in single sentence input tasks.\n",
    "    - Cons: adds additional computation. Subword modeling without pretrained word embedding can be biased since it does not take the meaning of words into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pointwise Mutual Information [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we introduced __pointwise mutual information__ (PMI), which addresses the issue of normalization removing information about absolute magnitudes of counts. The PMI for word $\\times$ context pair $(w,c)$ is \n",
    "\n",
    "$$\\log\\left(\\frac{P(w,c)}{P(w) \\cdot P(c)}\\right)$$\n",
    "\n",
    "with $\\log(0) = 0$. This is a measure of how far that cell's value deviates from what we would expect given the row and column sums for that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Implement `pmi`, a function which takes in a co-occurence matrix and returns the matrix with PMI normalization applied. [15 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat,ppmi= False):\n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"    \n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    total_count = np.sum(mat)\n",
    "    word_prob = np.sum(mat,axis = 1)/total_count\n",
    "    context_prob = np.sum(mat, axis =0)/total_count    \n",
    "    pc_prob = mat/total_count\n",
    "    pmi_mat = np.zeros((len(word_prob),len(context_prob)))\n",
    "    for i in range(len(word_prob)):\n",
    "        for j in range(len(context_prob)):\n",
    "            if pc_prob[i][j]/(word_prob[i]*context_prob[j]) ==0:\n",
    "                pmi_mat[i][j] == 0\n",
    "            else:\n",
    "                if ppmi == True:\n",
    "                    pmi_mat[i][j] = max(0.0, np.log2(pc_prob[i][j]/(word_prob[i]*context_prob[j])))# use base-2 log\n",
    "                else:\n",
    "                    pmi_mat[i][j] = np.log2(pc_prob[i][j]/(word_prob[i]*context_prob[j]))\n",
    "                \n",
    "     \n",
    "    return pmi_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PMI to the co-occurence matrix computed above with `context_size=1`. What is the PMI between the words \"the\" and \"end\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting words in 0.77883\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 28.53604\n"
     ]
    }
   ],
   "source": [
    "coocur_mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evashuyuwang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/evashuyuwang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI between 'the' and 'end' 3.057042629431626\n"
     ]
    }
   ],
   "source": [
    "coocur_mat_pmi = pmi(coocur_mat)\n",
    "print(\"PMI between 'the' and 'end'\",coocur_mat_pmi[tok2idx[\"the\"]][tok2idx[\"end\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. We also consider an extension of PMI, positive PMI (PPMI), that maps all negative PMI values to 0.0 ([Levy and Goldberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization)). \n",
    "Write `ppmi`, which is the same as `pmi` except it applies PPMI instead of PMI (feel free to implement it as an option of `pmi`). What is the PMI of the words \"the\" and \"start\"? The PPMI? [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of 'the' 2\n",
      "index of 'start' 670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evashuyuwang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/evashuyuwang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/evashuyuwang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI between 'the' and 'start' 0.98517981040929\n",
      "PPMI between 'the' and 'start' 0.98517981040929\n"
     ]
    }
   ],
   "source": [
    "# PPMI is implemented as an option of pmi\n",
    "\n",
    "print(\"index of 'the'\", tok2idx[\"the\"])\n",
    "print(\"index of 'start'\", tok2idx[\"start\"])\n",
    "\n",
    "coocur_mat_pmi = pmi(coocur_mat, ppmi = False)\n",
    "coocur_mat_ppmi = pmi(coocur_mat, ppmi = True)\n",
    "\n",
    "print(\"PMI between 'the' and 'start'\",coocur_mat_pmi[2][670])\n",
    "print(\"PPMI between 'the' and 'start'\",coocur_mat_ppmi[2][670])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing PMI [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Consider the matrix `np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])`. Reweight this matrix using `ppmi`. (i) What is the value obtained for cell `[0,0]`, and (ii) give a brief description for what is likely problematic about this value. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.32120693 0.         0.        ]\n",
      " [0.         0.         0.41532607]\n",
      " [0.32156765 0.73732631 0.        ]]\n",
      "value obtained for cell [0,0]: 2.3212069276437086\n"
     ]
    }
   ],
   "source": [
    "mat_3 = np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])\n",
    "mat_3_ppmi = pmi(mat_3,ppmi=True)\n",
    "print(mat_3_ppmi)\n",
    "print(\"value obtained for cell [0,0]:\",mat_3_ppmi[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The PMI value is biased towards infrequent events, which means very rare words have high PMI values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Give a suggestion for dealing with the problematic value and explain why it deals with this. Demonstrate your suggestion empirically [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.32034343 0.         0.        ]\n",
      " [0.         0.         0.41539784]\n",
      " [0.32106423 0.73682146 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Use Laplace smoothing to reduce the bias of PMI towards infrequent term.\n",
    "mat_laplace = mat_3 + 1 # Adding a constant positive value to raw frequencies before calculating the probability.\n",
    "ppmi_laplace = pmi(mat_laplace,ppmi=True)\n",
    "print(ppmi_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As shown in the results above, by using Laplace smoothing, the value obtained for cell [0,0] is smaller ( and less biased ). This is because the magnitued of the push by the constant depends on the raw frequency. Small frequency is adjusted on a larger scale, larger frequency is adjusted by smaller push."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Consider starting with a word-word co-occurence matrix and applied PMI to this matrix. (i) Which of the following describe the resulting vectors: sparse, dense, high-dimensional, low-dimensional (ii) If you wanted the opposite style of representation, what could you do? [5 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(i)_ Sparse, high-dimensional\n",
    "\n",
    "_(ii)_ Dimensionality reduction: SVD, LSA, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Analogy Evaluation [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word analogies provide another kind of evaluation for distributed representations. Here, we are given three vectors A, B, and C, in the relationship\n",
    "\n",
    "_A is to B as C is to __ _\n",
    "\n",
    "and asked to identify the fourth that completes the analogy. These analogies are by and large substantially easier than the classic brain-teaser analogies that used to appear on tests like the SAT, but it's still an interesting, demanding\n",
    "task. \n",
    "\n",
    "The core idea is that we make predictions by creating the vector\n",
    "\n",
    "$$(A - B) + C$$ \n",
    "\n",
    "and then ranking all vectors based on their distance from this new vector, choosing the closest as our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Implement the function `analogy_completion`. [9 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_completion(a, b, c, mat_dic = glove_vecs,distfunc=cosine):\n",
    "    \"\"\"Compute ? in \n",
    "    a is to b as c is to ? \n",
    "    as the closest to (b-a) + c\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "    # compare all vectors to (a-b)+c\n",
    "    tok_list = list(mat_dic.keys()) \n",
    "    for x in (a,b,c):\n",
    "        if x not in tok_list:\n",
    "            raise ValueError('%s is not in this VSM' % x)\n",
    "    \n",
    "    a_vec = mat_dic[a]\n",
    "    b_vec = mat_dic[b]\n",
    "    c_vec = mat_dic[c]\n",
    "    \n",
    "    new_vec =  (b_vec-a_vec) + c_vec  \n",
    "    dists = [(tok_list[i], distfunc(new_vec, mat_dic[tok_list[i]])) for i in range(len(mat_dic)) if tok_list[i] not in (a,b,c)]\n",
    "    \n",
    "    return sorted(dists, key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Our simple word embeddings likely won't perform well on this task. Let's instead look at some high quality pretrained word embeddings. Write code to load 300-dimensional [GloVe word embeddings](http://nlp.stanford.edu/data/glove.840B.300d.zip) trained on 840B tokens. Each line of the file is formatted as a word followed by 300 floats that make up its corresponding word embedding (all space delimited). The entries of GloVe word embeddings are not counts, but instead are learned via machine learning. Use your `analogy_completion` code to complete the following analogies using the GloVe word embeddings. [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Beijing\" is to \"China\" as \"Paris\" is to ?\n",
    "- \"gold\" is to \"first\" as \"silver\" is to ?\n",
    "- \"Italian\" is to \"mozzarella\" as \"American\" is to ?\n",
    "- \"research\" is to \"fun\" as \"engineering\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_file, n_vecs=50000): #top 50000 tokens\n",
    "    \"\"\" \"\"\"\n",
    "    tok2vec = {}\n",
    "    with open(glove_file, 'r') as glove_fh:\n",
    "        for i, row in enumerate(glove_fh):\n",
    "            word, vec = row.split(' ', 1)\n",
    "            tok2vec[word] = np.array([float(n) for n in vec.split(' ')])\n",
    "            if i >= n_vecs:\n",
    "                break\n",
    "    return tok2vec\n",
    "glove_file = \"glove.840B.300d.txt\"\n",
    "glove_vecs = load_glove(glove_file, n_vecs=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 0.27706330530641043)\n",
      "('second', 0.3024389409663598)\n",
      "('cheddar', 0.4171742770336405)\n",
      "('awesome', 0.5027277614731709)\n"
     ]
    }
   ],
   "source": [
    "# implement analogy_completion\n",
    "# use GloVe word embeddings instead of co-occurence counts\n",
    "r1 = analogy_completion(\"Beijing\", \"China\", \"Paris\", mat_dic= glove_vecs,distfunc=cosine)\n",
    "print(r1[0])\n",
    "r2 = analogy_completion(\"gold\", \"first\", \"silver\", mat_dic= glove_vecs,distfunc=cosine)\n",
    "print(r2[0])\n",
    "r3 = analogy_completion(\"Italian\",\"mozzarella\",\"American\", mat_dic= glove_vecs,distfunc=cosine)\n",
    "print(r3[0])\n",
    "r4 = analogy_completion(\"research\",\"fun\", \"engineering\", mat_dic= glove_vecs,distfunc=cosine)\n",
    "print(r4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Let's get a more quantitative, aggregate sense of the quality of GloVe embeddings. Load the analogies from `gram6-nationality-adjective.txt` and evaluate GloVe embeddings. Report the mean reciprocal rank of the correct answer (the last word on each line) for each analogy. [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_evaluation(glove_vecs, test_file, verbose=False):\n",
    "    \"\"\"Basic analogies evaluation for a file `src_filename `\n",
    "    in `question-data/`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    mat : 2d np.array\n",
    "        The VSM being evaluated.\n",
    "        \n",
    "    rownames : list of str\n",
    "        The names of the rows in `mat`.\n",
    "        \n",
    "    src_filename : str\n",
    "        Basename of the file to be evaluated. It's assumed to be in\n",
    "        `vsmdata_home`/question-data.\n",
    "        \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure \n",
    "        between 1d vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (float, float)\n",
    "        The first is the mean reciprocal rank of the predictions and \n",
    "        the second is the accuracy of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = [line.split() for line in open(test_file).read().splitlines()]\n",
    "    ranks = []\n",
    "    acc = defaultdict(int)\n",
    "    preds = []\n",
    "    n_exs = 0\n",
    "    tok_list = list(glove_vecs.keys())\n",
    "    for datum in data:\n",
    "        a = datum[0]\n",
    "        b = datum[1]\n",
    "        c = datum[2]\n",
    "        trg = datum[3]\n",
    "        if all(x in tok_list for x in (a,b,c,trg)): # only include rows with all tokens in the token list\n",
    "            preds = analogy_completion(a, b, c, mat_dic = glove_vecs,distfunc=cosine)\n",
    "            acc[preds[0][0] == trg] += 1\n",
    "            predicted_words, _ = zip(*preds)\n",
    "            ranks.append(predicted_words.index(trg))\n",
    "            n_exs += 1\n",
    "        \n",
    "    # Return the mean reciprocal rank\n",
    "    mrr = np.mean(1.0/(np.array(ranks)+1))\n",
    "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(data)))\n",
    "    return (mrr,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 234 of 1599 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9533545848238831, defaultdict(int, {True: 219, False: 15}))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_evaluation(glove_vecs, \"gram6-nationality-adjective.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
